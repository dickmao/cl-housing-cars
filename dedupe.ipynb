{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, errno, operator, re, sys, subprocess, signal\n",
    "#sys.path.append(os.path.join(wdir, '..', 'flaubert'))\n",
    "#import flaubert.punkt\n",
    "from __future__ import division\n",
    "from IPython.core.debugger import Pdb\n",
    "import pickle\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import enchant\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams,ConditionalFreqDist,FreqDist,pos_tag,pos_tag_sents\n",
    "from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization\n",
    "from gensim.utils import lemmatize\n",
    "from nltk import collocations, association, text, tree\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.similarities.docsim import SparseMatrixSimilarity\n",
    "from reader import Json100CorpusReader\n",
    "import itertools, shutil\n",
    "from collections import Counter\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "#from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#from sklearn.learning_curve import learning_curve\n",
    "\n",
    "import re\n",
    "from lxml import etree\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (\n",
    "    DependencyGraph, ProjectiveDependencyParser, NonprojectiveDependencyParser)\n",
    "from nltk.parse.malt import MaltParser as MaltParser\n",
    "\n",
    "from nltk.corpus import dependency_treebank as dt\n",
    "from nltk.corpus import treebank_raw\n",
    "from nltk.corpus import treebank\n",
    "from pickle import load\n",
    "from nltk.parse import stanford\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "\n",
    "from glob import iglob\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "from os.path import getmtime, join, realpath\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tla = ['abo', 'sub', 'apa', 'cto']\n",
    "NAME = os.environ.get('NAME') or 'dmoz'\n",
    "wdir = os.path.expanduser('~/scrapy')\n",
    "os.environ['CLASSPATH'] = join(wdir, 'lib')\n",
    "odir = join(wdir, NAME)\n",
    "os.chdir(odir)\n",
    "tm_marker1 = getmtime(join(odir, 'marker1'))\n",
    "lookback = 2\n",
    "jsons = [f for f in os.listdir(odir) if re.search(r'\\.json$', f) and getmtime(f) >= tm_marker1 - 86400*lookback]\n",
    "craigcr = Json100CorpusReader(odir, jsons)\n",
    "coords = craigcr.coords()\n",
    "links = craigcr.links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_dict(crlist):\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = set()\n",
    "    class nouner(object):\n",
    "        def __init__(self, cr):\n",
    "            self.cr = cr\n",
    "        def __iter__(self):\n",
    "            for doc in self.cr.sents():\n",
    "                yield doc\n",
    "                # for t in list(parser.parse(sent)):\n",
    "                #     for p in t.productions():\n",
    "                #         if p.is_lexical():\n",
    "                #             if p.lhs() == 'NN':\n",
    "                #                 yield 'NN'\n",
    "                #             else:\n",
    "                #                 print p.rhs()\n",
    "                #                 yield p.rhs()\n",
    "    # corpora.Dictionary is a static method of gensim.corpora\n",
    "    # it establishes the base of operations numbering the vocab,\n",
    "    # and you feed it strings such as doc2bow(feedit) produces a sparse vector.\n",
    "    dictionary = corpora.Dictionary(list(itertools.chain.from_iterable([list(nouner(cr)) for cr in crlist])))\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "                if stopword in dictionary.token2id]\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "    mispelled_ids = [dictionary.token2id[word] for word in dictionary.values() if not checker.check(word)]\n",
    "    dictionary.filter_tokens(stop_ids + once_ids + mispelled_ids)\n",
    "    \n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', 'RB', 'RBR', 'RBS', 'NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBZ' ])\n",
    "    contextual_ids = [dictionary.token2id[k[0][0]] for k in pos_tag_sents([[w] for w in dictionary.values()]) if k[0][1] not in myset]\n",
    "    \n",
    "#    dictionary.filter_tokens(contextual_ids)\n",
    "    dictionary.compactify()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class i2what(object):\n",
    "    def __init__(self, arr):\n",
    "        self._i2w = arr\n",
    "    def __len__(self):\n",
    "        return len(self._i2w)    \n",
    "    def q_dupe(self, i):\n",
    "        return self._i2w[i] < 0\n",
    "    def __getitem__(self, i):\n",
    "        return abs(self._i2w[i])\n",
    "    def __iter__(self):\n",
    "        for i in self._i2w:\n",
    "            if self.q_dupe(i):\n",
    "                next\n",
    "            else:\n",
    "                yield self[i]\n",
    "        \n",
    "def CorpusDedupe(cr, dict):\n",
    "    corpus = [dict.doc2bow(doc[:30]) for doc in list(cr)]\n",
    "    i2text = np.arange(1,len(corpus)+1,1)\n",
    "    i2loc = np.arange(1,len(corpus)+1,1)\n",
    "    index = SparseMatrixSimilarity(corpus, num_features=len(dict.keys()))\n",
    "    for i, z in enumerate(zip(index, coords)):\n",
    "        if i2text[i] > 0:\n",
    "            negated = -i2text[i]\n",
    "            for j, sim in enumerate(z[0][i+1:]):\n",
    "                if sim > .99:\n",
    "\t\t    i2text[i] = i2text[i+1+j] = negated\n",
    "        if i2loc[i] > 0 and None not in z[1]:\n",
    "            ci = z[1]\n",
    "            negated = -i2loc[i]\n",
    "            for j, cj in enumerate(coords[i+1:]):\n",
    "                if ci == cj:\n",
    "                    i2loc[i] = i2loc[i+1+j] = negated\n",
    "    return i2what(i2text), i2what(i2loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# establish vocab numbering\n",
    "dict = make_dict([craigcr])\n",
    "# corpus = collection of bow sparse vectors\n",
    "i2text, i2loc = CorpusDedupe(craigcr, dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, asin\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6372.8 # Earth radius in kilometers\n",
    "    dLat = radians(lat2 - lat1)\n",
    "    dLon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "\n",
    "    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    " \n",
    "    return R * c\n",
    "\n",
    "def within(coords):\n",
    "    if coords[0] is None or coords[1] is None:\n",
    "\treturn True\n",
    "    if NAME == \"dmoz\":\n",
    "\treturn float(coords[0]) < 40.796126\n",
    "    # que\n",
    "    elif NAME == \"que\":\n",
    "\tkm = haversine(40.748604, -73.937523, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 4\n",
    "    # sf\n",
    "    elif NAME == \"sfc\":\n",
    "\tkm = haversine(37.779076, -122.397501, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 1.5\n",
    "    # berkeley\n",
    "    elif NAME == \"eby\":\n",
    "\tkm = haversine(37.871454, -122.298115, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 3\n",
    "    # milbrae\n",
    "    #    km = haversine(37.600122, -122.386914, float(coords[0]), float(coords[1]))\n",
    "    # daly city\n",
    "    #    km2 = haversine(37.687915, -122.472452, float(coords[0]), float(coords[1]))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def filterByWord(vOfv, listedby):\n",
    "    scam = re.compile(\"!!+|leasebreak|dalia|greystar\", re.IGNORECASE)\n",
    "    pronouns = re.compile(\"^(i|me|mine|our|he|she|they|their|we|my|his|her|myself|himself|herself|themselves)$\", re.IGNORECASE)\n",
    "    ok = bool(listedby) # if sublet (not listedby) then default to suspicious\n",
    "    for w in [w for sent in vOfv for w in sent]:\n",
    "\tif scam.search(w):\n",
    "\t    return False\n",
    "\tif pronouns.search(w):\n",
    "\t    ok = True\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def nPara(raw):\n",
    "    lines = raw.split('\\n')\n",
    "    wstart = next( (i for (i,l) in enumerate(lines) if re.search(r\"\\S\", l) ), 0)\n",
    "    wend = len(lines) - next( (i for (i,l) in enumerate(reversed(lines)) if re.search(r\"\\S\", l) ), 0)\n",
    "    result = 0\n",
    "    inPara = False\n",
    "    for l in lines[wstart:wend]:\n",
    "        if re.search(r\"\\S\", l):\n",
    "            if not inPara:\n",
    "                inPara = True\n",
    "                result += 1\n",
    "        elif inPara:\n",
    "            inPara = False\n",
    "    return result\n",
    "    \n",
    "def numSents(vOfv):\n",
    "    return len(vOfv)\n",
    "\n",
    "def numYell(vOfv):\n",
    "    return sum([1 for v in vOfv for w in v if re.search(\"[A-Z]{3}\", w)])\n",
    "\n",
    "def numWords(vOfv):\n",
    "    return sum([len(v) for v in vOfv])\n",
    "\n",
    "def numNonAscii(vOfv):\n",
    "    return sum([1 for v in vOfv for w in v if any(ord(char) > 127 for char in w)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "listedby = craigcr.attrs_matching(r'[lL]isted')\n",
    "listedby = [re.split(r':\\s*', i, 1).pop() if i else None for i in listedby]\n",
    "oklistedby = set()\n",
    "for pair in Counter(listedby).iteritems():\n",
    "    if not pair[0]:\n",
    "\toklistedby.add(pair[0])\n",
    "    elif pair[1] == 1:\n",
    "\tif not re.search(r\"no fee|contact|apartments|apts|for all|llc|to view|===+|----+|\\*\\*\\*+|\\.\\.\\.+|xxxx+|best|mark a|marc a|rentals|real|estate\", pair[0], re.IGNORECASE):\n",
    "\t    if len(pair[0].split()) < 5:\n",
    "\t\toklistedby.add(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "odoms = craigcr.attrs_matching(r'[oO]dom')\n",
    "odoms = [re.split(r':\\s*', i, 1).pop() if i else None for i in odoms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(join(odir, 'files'))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.ENOENT:\n",
    "        raise\n",
    "try:\n",
    "    os.makedirs(join(odir, 'files'))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "lst,rej = [],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i,z in enumerate(zip(craigcr.docs(), craigcr.raw(newlines_are_periods=True))):\n",
    "    listing = '{} {} {}'.format(' '.join([word.encode('utf-8') for sent in z[0] for word in sent][0:50]), \\\n",
    "                                links[i], \\\n",
    "                                re.sub(r'\\s+', ' ', listedby[i]) if listedby[i] else \"Actual Person?\")\n",
    "    # filter in order of increasing time complexity\n",
    "    if i2text.q_dupe(i):\n",
    "        rej.append(\"dupe {}\".format(listing))\n",
    "        continue\n",
    "    if listedby[i] not in oklistedby:\n",
    "        rej.append(\"listedby {}\".format(listing))\n",
    "        continue\n",
    "    if odoms[i] and int(odoms[i]) > 160000:\n",
    "        rej.append(\"miles {}\".format(listing))\n",
    "        continue\n",
    "    if not within(coords[i]):\n",
    "        rej.append(\"toofar {}\".format(listing))\n",
    "        continue\n",
    "    if not filterByWord(z[0], listedby[i]):\n",
    "        rej.append(\"byword {}\".format(listing))\n",
    "        continue\n",
    "        \n",
    "    nw=numWords(z[0])\n",
    "    ns=numSents(z[0])\n",
    "    wps=float(nw/ns) if ns else 0.0\n",
    "    np=nPara(z[1])\n",
    "    spp=float(len(z[0])/np) if np else 0.0\n",
    "    yr=float(numYell(z[0])/nw) if nw else 0.0\n",
    "    nna=numNonAscii(z[0])\n",
    "\n",
    "    if nna > 3 or wps <= 10 or spp <= 1.0 or yr > 0.1:\n",
    "        rej.append(\"garbage {}\".format(listing))\n",
    "        continue\n",
    "    lst.append(\"{} np={} wps={} spp={:.2f}\".format(listing, np, int(wps), spp))\n",
    "    tla_link = re.findall(r\"({0})/(?:[^/]+/)*?(\\d+).html\".format('|'.join(tla)), links[i])[-1]\n",
    "    with open(join(odir, \"files\", \"{}-{}\".format(tla_link[0], tla_link[1])), \"w\") as f:\n",
    "        f.write(z[1].encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def dump(file, tla, lines):\n",
    "    mark = {}\n",
    "    re_html = r\"{0}/(?:[^/]+/)*?(\\d+).html\"\n",
    "    for type in tla:\n",
    "        try:\n",
    "            with open(join(odir, 'marker0'), 'r') as f0:\n",
    "                mark[type] = max(['0'] + [re.findall(re_html.format(type), line)[-1] for line in f0.readlines() if re.findall(re_html.format(type), line)])\n",
    "        except (IOError, IndexError) as e:\n",
    "            mark[type] = '0'\n",
    "\n",
    "    with open(file, 'w+') as f1:\n",
    "        for type in tla:\n",
    "            for listing in lines:\n",
    "                try:\n",
    "                    id = re.findall(re_html.format(type), listing)[-1]\n",
    "                    if id > mark[type]:\n",
    "                        f1.write(listing + '\\n\\n')\n",
    "                except IndexError as e:\n",
    "                    pass    \n",
    "dump(join(odir, 'digest'), tla, lst)\n",
    "dump(join(odir, 'reject'), tla, rej)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f74f1a1add0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientcmd = \"gradle -p {} client -Pcargs=\\\"['-file', '{}', '-outputDirectory', '{}']\\\"\".format(join(wdir, \"..\", \"CoreNLP\"), join(odir, \"files\"), join(odir, \"files\"))\n",
    "servercmd = \"gradle -p {} server\".format(join(wdir, \"..\", \"CoreNLP\"))\n",
    "if getpass.getuser() != 'ubuntu':\n",
    "    subprocess.Popen(servercmd, shell=True, stdout=subprocess.PIPE, preexec_fn=os.setsid)\n",
    "    subprocess.Popen(clientcmd, shell=True, stdout=subprocess.PIPE)\n",
    "# os.system(\"wget localhost:9000/shutdown?key=$(cat /var/tmp/corenlp.shutdown) -O -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "dedupe.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
