{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, errno, operator, re, sys\n",
    "#sys.path.append(os.path.join(wdir, '..', 'flaubert'))\n",
    "#import flaubert.punkt\n",
    "from __future__ import division\n",
    "from IPython.core.debugger import Pdb\n",
    "import pickle\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import enchant\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams,ConditionalFreqDist,FreqDist,pos_tag,pos_tag_sents\n",
    "from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization\n",
    "from gensim.utils import lemmatize\n",
    "from nltk import collocations, association, text, tree\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.similarities.docsim import SparseMatrixSimilarity\n",
    "from reader import Json100CorpusReader\n",
    "import itertools, shutil\n",
    "from collections import Counter\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "#from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#from sklearn.learning_curve import learning_curve\n",
    "\n",
    "import re\n",
    "from lxml import etree\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (\n",
    "    DependencyGraph, ProjectiveDependencyParser, NonprojectiveDependencyParser)\n",
    "from nltk.parse.malt import MaltParser as MaltParser\n",
    "\n",
    "from nltk.corpus import dependency_treebank as dt\n",
    "from nltk.corpus import treebank_raw\n",
    "from nltk.corpus import treebank\n",
    "from pickle import load\n",
    "from nltk.parse import stanford\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "\n",
    "from glob import iglob\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "from os.path import getmtime, join, realpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tla = ['abo', 'sub', 'apa', 'cto']\n",
    "NAME = os.environ.get('NAME') or 'dmoz'\n",
    "wdir = os.path.expanduser('~/scrapy')\n",
    "os.environ['CLASSPATH'] = join(wdir, 'lib')\n",
    "odir = join(wdir, NAME)\n",
    "os.chdir(odir)\n",
    "tm_marker1 = getmtime(join(odir, 'marker1'))\n",
    "lookback = 2\n",
    "jsons = [f for f in os.listdir(odir) if re.compile(r'\\.json$').search(f) and getmtime(f) > tm_marker1 - 86400*lookback]\n",
    "craigcr = Json100CorpusReader(odir, jsons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_dict(crlist):\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = set()\n",
    "    class nouner(object):\n",
    "        def __init__(self, cr):\n",
    "            self.cr = cr\n",
    "        def __iter__(self):\n",
    "            for doc in self.cr.sents():\n",
    "                yield doc\n",
    "                # for t in list(parser.parse(sent)):\n",
    "                #     for p in t.productions():\n",
    "                #         if p.is_lexical():\n",
    "                #             if p.lhs() == 'NN':\n",
    "                #                 yield 'NN'\n",
    "                #             else:\n",
    "                #                 print p.rhs()\n",
    "                #                 yield p.rhs()\n",
    "    # corpora.Dictionary is a static method of gensim.corpora\n",
    "    # it establishes the base of operations numbering the vocab,\n",
    "    # and you feed it strings such as doc2bow(feedit) produces a sparse vector.\n",
    "    dictionary = corpora.Dictionary(list(itertools.chain.from_iterable([list(nouner(cr)) for cr in crlist])))\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "                if stopword in dictionary.token2id]\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "    mispelled_ids = [dictionary.token2id[word] for word in dictionary.values() if not checker.check(word)]\n",
    "    dictionary.filter_tokens(stop_ids + once_ids + mispelled_ids)\n",
    "    \n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', 'RB', 'RBR', 'RBS', 'NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBZ' ])\n",
    "    contextual_ids = [dictionary.token2id[k[0][0]] for k in pos_tag_sents([[w] for w in dictionary.values()]) if k[0][1] not in myset]\n",
    "    \n",
    "#    dictionary.filter_tokens(contextual_ids)\n",
    "    dictionary.compactify()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def CorpusDedupe(cr, dict):\n",
    "    corpus = [dict.doc2bow(doc[:30]) for doc in list(cr)]\n",
    "    all = set(range(0,len(corpus)))\n",
    "    index = SparseMatrixSimilarity(corpus,num_features=len(dict.keys()))\n",
    "    marked = set()\n",
    "    for s, similarities in enumerate(index):\n",
    "        if s in marked:\n",
    "            continue\n",
    "        for i, item in enumerate(similarities[s+1:]):\n",
    "            if item > .99:\n",
    "\t\tmarked.add(s)\n",
    "                marked.add(s+1+i)\n",
    "\n",
    "    # you recover the original indexing through marked\n",
    "    corpus = [dict.doc2bow(doc) for doc in list(cr)]\n",
    "    for m in sorted(marked, reverse=True):\n",
    "        del corpus[m]\n",
    "    return (all.difference(marked), corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# establish vocab numbering\n",
    "dict = make_dict([craigcr])\n",
    "# corpus = collection of bow sparse vectors\n",
    "(marked, corpus) = CorpusDedupe(craigcr, dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, asin\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6372.8 # Earth radius in kilometers\n",
    "    dLat = radians(lat2 - lat1)\n",
    "    dLon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "\n",
    "    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    " \n",
    "    return R * c\n",
    "\n",
    "def within(coords):\n",
    "    if coords[0] is None or coords[1] is None:\n",
    "\treturn True\n",
    "    if NAME == \"dmoz\":\n",
    "\treturn float(coords[0]) < 40.796126\n",
    "    # que\n",
    "    elif NAME == \"que\":\n",
    "\tkm = haversine(40.748604, -73.937523, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 4\n",
    "    # sf\n",
    "    elif NAME == \"sfc\":\n",
    "\tkm = haversine(37.779076, -122.397501, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 1.5\n",
    "    # berkeley\n",
    "    elif NAME == \"eby\":\n",
    "\tkm = haversine(37.871454, -122.298115, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 3\n",
    "    # milbrae\n",
    "    #    km = haversine(37.600122, -122.386914, float(coords[0]), float(coords[1]))\n",
    "    # daly city\n",
    "    #    km2 = haversine(37.687915, -122.472452, float(coords[0]), float(coords[1]))\n",
    "    return True\n",
    "\n",
    "coords = craigcr.coords()\n",
    "odoms = craigcr.attrs_matching(r'[oO]dom')\n",
    "odoms = [re.split(r':\\s*', i, 1).pop() if i else None for i in odoms]\n",
    "outside = set()\n",
    "for ci in marked:\n",
    "    if not within(coords[ci]):\n",
    "\toutside.add(ci)\n",
    "    if odoms[ci] and int(odoms[ci]) > 160000:\n",
    "        outside.add(ci)\n",
    "            \n",
    "marked = marked.difference(outside)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sameloc = set()\n",
    "for ci in marked:\n",
    "    if coords[ci][0] is None:\n",
    "        continue\n",
    "    if ci in sameloc:\n",
    "        continue\n",
    "    # for j,c2 in enumerate(list(marked[type])[i+1:]):\n",
    "    # \tif coords[c2] == coords[c]:\n",
    "    # \t    sameloc.add(i)\n",
    "    # \t    sameloc.add(i+1+j)\n",
    "    c1 = coords[ci]\n",
    "    for cj,c2 in enumerate(coords):\n",
    "        if ci == cj:\n",
    "            continue\n",
    "        if c2 == c1:\n",
    "            sameloc.add(ci)\n",
    "            sameloc.add(cj)\n",
    "            break\n",
    "# same lat and long is okay ... comment below\n",
    "# but leasebreak piles into same coord\n",
    "# marked[type] = marked[type].difference(sameloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def filter(vOfv, listedby):\n",
    "    ok = bool(listedby) # if sublet (not listedby) then default to suspicious\n",
    "    for w in [w for sent in vOfv for w in sent]:\n",
    "#\tif w in set(['broker', 'fee', 'fully', 'brokerage', 'leasebreak', 'female', 'rentsfnow', 'hivesf', 'heritage', 'realty', 'application', 'yr', 'year', '12', ]):\n",
    "#\t    return False\n",
    "\tif re.compile(\"!!+|leasebreak|dalia|greystar\", re.IGNORECASE).search(w):\n",
    "\t    return False\n",
    "\tif re.compile(\"^(i|me|mine|our|he|she|they|their|we|my|his|her|myself|himself|herself|themselves)$\", re.IGNORECASE).search(w):\n",
    "\t    ok = True\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "listedby = craigcr.attrs_matching(r'[lL]isted')\n",
    "listedby = [re.split(r':\\s*', i, 1).pop() if i else None for i in listedby]\n",
    "filtListedBy = set()\n",
    "for pair in Counter(listedby).iteritems():\n",
    "    if not pair[0]:\n",
    "\tfiltListedBy.add(pair[0])\n",
    "    elif pair[1] == 1:\n",
    "\tif not re.compile(\"no fee|contact|apartments|apts|for all|llc|to view|===+|----+|\\*\\*\\*+|\\.\\.\\.+|xxxx+|best|mark a|marc a|rentals|real|estate\", re.IGNORECASE).search(pair[0]):\n",
    "\t    if len(pair[0].split()) < 5:\n",
    "\t\tfiltListedBy.add(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mark = {}\n",
    "re_html = r\"{0}/(?:[^/]+/)*?(\\d+).html\"\n",
    "for type in tla:\n",
    "    try:\n",
    "\twith open(join(odir, 'marker0'), 'r') as f0:\n",
    "\t    mark[type] = max(['0'] + [re.findall(re_html.format(type), line)[-1] for line in f0.readlines() if re.findall(re_html.format(type), line)])\n",
    "    except (IOError, IndexError) as e:\n",
    "\tmark[type] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "links = craigcr.links()\n",
    "try:\n",
    "    shutil.rmtree(join(odir, 'files'))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.ENOENT:\n",
    "        raise\n",
    "try:\n",
    "    os.makedirs(join(odir, 'files'))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "\n",
    "re_fileid = r\"({0})/(?:[^/]+/)*?(\\d+).html\".format('|'.join(tla))\n",
    "for (i, s) in enumerate(craigcr.raw()):\n",
    "    g = re.findall(re_fileid, links[i])[-1]\n",
    "    with open(join(odir, \"files\", \"{0}-{1}\".format(g[0], g[1])), \"w\") as f:\n",
    "        f.write(s.encode('utf-8'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# \"gradle client -Pcargs=\\\"['-file', '{0}']\\\"\".format(join(odir, \"files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def nPara(raw):\n",
    "    lines = raw.split('\\n')\n",
    "    wstart = next( (i for (i,l) in enumerate(lines) if re.search(r\"\\S\", l) ), 0)\n",
    "    wend = len(lines) - next( (i for (i,l) in enumerate(reversed(lines)) if re.search(r\"\\S\", l) ), 0)\n",
    "    result = 0\n",
    "    inPara = False\n",
    "    for l in lines[wstart:wend]:\n",
    "        if re.search(r\"\\S\", l):\n",
    "            if not inPara:\n",
    "                inPara = True\n",
    "                result += 1\n",
    "        elif inPara:\n",
    "            inPara = False\n",
    "    return result\n",
    "    \n",
    "def numSents(vOfv):\n",
    "    return len(vOfv)\n",
    "\n",
    "def numYell(vOfv):\n",
    "    return sum([1 for v in vOfv for w in v if re.search(\"[A-Z]{3}\", w)])\n",
    "\n",
    "def numWords(vOfv):\n",
    "    return sum([len(v) for v in vOfv])\n",
    "\n",
    "def numNonAscii(vOfv):\n",
    "    return sum([1 for v in vOfv for w in v if any(ord(char) > 127 for char in w)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i,z in enumerate(zip(craigcr.docs(), craigcr.raw())):\n",
    "    nw=numWords(z[0])\n",
    "    ns=numSents(z[0])\n",
    "    wps=float(nw/ns) if ns else 0.0\n",
    "    np=nPara(z[1])\n",
    "    spp=float(len(z[0])/np) if np else 0.0\n",
    "    yr=float(numYell(z[0])/nw) if nw else 0.0\n",
    "    nna=numNonAscii(z[0])\n",
    "    if i in set(marked) and filter(z[0], listedby[i]) and\\\n",
    "        listedby[i] in filtListedBy and\\\n",
    "        nna < 3 and wps > 10 and spp > 1.0 and yr < 0.1:\n",
    "        lst.append('%s %s %s np=%d wps=%d spp=%f' % (' '.join([word for sent in z[0] for word in sent][0:50]), links[i], re.sub(r'\\s+', ' ', listedby[i]) if listedby[i] else \"Actual Person?\", np, wps, spp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open(join(odir, 'digest'), 'w+') as f1:\n",
    "    for type in tla:\n",
    "        for listing in lst:\n",
    "\t    try:\n",
    "                id = re.findall(re_html.format(type), listing)[-1]\n",
    "\t\tif id > mark[type]:\n",
    "\t\t    f1.write(listing.encode('ascii', 'ignore') + '\\n\\n')\n",
    "\t    except IndexError as e:\n",
    "\t\tpass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "dedupe.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
