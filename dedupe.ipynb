{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.core.debugger import Pdb\n",
    "import pickle\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import enchant\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams,ConditionalFreqDist,FreqDist,pos_tag,pos_tag_sents\n",
    "from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization\n",
    "from gensim.utils import lemmatize\n",
    "from nltk import collocations, association, text, tree\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.similarities.docsim import SparseMatrixSimilarity\n",
    "from reader import Json100CorpusReader as js100\n",
    "from reader import BlogCorpusReader\n",
    "import itertools\n",
    "from collections import Counter\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "#from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#from sklearn.learning_curve import learning_curve\n",
    "import re\n",
    "\n",
    "from lxml import etree\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (\n",
    "    DependencyGraph, ProjectiveDependencyParser, NonprojectiveDependencyParser)\n",
    "from nltk.parse.malt import MaltParser as MaltParser\n",
    "\n",
    "from nltk.corpus import dependency_treebank as dt\n",
    "from nltk.corpus import treebank_raw\n",
    "from nltk.corpus import treebank\n",
    "import os\n",
    "from pickle import load\n",
    "from nltk.parse import stanford\n",
    "import operator\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "\n",
    "from glob import iglob\n",
    "from StringIO import StringIO\n",
    "from os import listdir\n",
    "from os.path import join as pathjoin\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "import re\n",
    "\n",
    "tla = ['abo', 'sub', 'apa']\n",
    "NAME = os.environ.get('NAME') or 'dmoz'\n",
    "wdir = os.path.expanduser('~/scrapy')\n",
    "os.environ['CLASSPATH'] = pathjoin(wdir, 'lib')\n",
    "odir = pathjoin(wdir, NAME)\n",
    "os.chdir(odir)\n",
    "\n",
    "craigcr = js100(odir, r'marker1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_dict(crlist):\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = set()\n",
    "    class nouner(object):\n",
    "        def __init__(self, cr):\n",
    "            self.cr = cr\n",
    "        def __iter__(self):\n",
    "            for doc in self.cr.sents():\n",
    "                yield doc\n",
    "                # for t in list(parser.parse(sent)):\n",
    "                #     for p in t.productions():\n",
    "                #         if p.is_lexical():\n",
    "                #             if p.lhs() == 'NN':\n",
    "                #                 yield 'NN'\n",
    "                #             else:\n",
    "                #                 print p.rhs()\n",
    "                #                 yield p.rhs()\n",
    "    # corpora.Dictionary is a static method of gensim.corpora\n",
    "    # it establishes the base of operations numbering the vocab,\n",
    "    # and you feed it strings such as doc2bow(feedit) produces a sparse vector.\n",
    "    dictionary = corpora.Dictionary(list(itertools.chain.from_iterable([list(nouner(cr)) for cr in crlist])))\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "                if stopword in dictionary.token2id]\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "    mispelled_ids = [dictionary.token2id[word] for word in dictionary.values() if not checker.check(word)]\n",
    "    dictionary.filter_tokens(stop_ids + once_ids + mispelled_ids)\n",
    "    \n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'RB', 'RBR', 'RBS', 'UH', 'NNP', 'NNS',  ])\n",
    "    myset = set(['WRB', 'WP$', 'WP', 'WDT', 'TO', 'RP', 'PRP$', 'PRP', 'PDT', 'MD', 'IN', 'EX', 'DT', 'CC', 'UH', 'RB', 'RBR', 'RBS', 'NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBZ' ])\n",
    "    contextual_ids = [dictionary.token2id[k[0][0]] for k in pos_tag_sents([[w] for w in dictionary.values()]) if k[0][1] not in myset]\n",
    "    \n",
    "#    dictionary.filter_tokens(contextual_ids)\n",
    "    dictionary.compactify()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def CorpusDedupe(cr, dict):\n",
    "    corpus = [dict.doc2bow(doc[:30]) for doc in list(cr)]\n",
    "    all = set(range(0,len(corpus)))\n",
    "    index = SparseMatrixSimilarity(corpus,num_features=len(dict.keys()))\n",
    "    marked = set()\n",
    "    for s, similarities in enumerate(index):\n",
    "        if s in marked:\n",
    "            continue\n",
    "        for i, item in enumerate(similarities[s+1:]):\n",
    "            if item > .99:\n",
    "\t\tmarked.add(s)\n",
    "                marked.add(s+1+i)\n",
    "\n",
    "    # you recover the original indexing through marked\n",
    "    corpus = [dict.doc2bow(doc) for doc in list(cr)]\n",
    "    for m in sorted(marked, reverse=True):\n",
    "        del corpus[m]\n",
    "    return (all.difference(marked), corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# establish vocab numbering\n",
    "dict = make_dict([craigcr])\n",
    "# corpus = collection of bow sparse vectors\n",
    "(marked, corpus) = CorpusDedupe(craigcr, dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, asin\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6372.8 # Earth radius in kilometers\n",
    "    dLat = radians(lat2 - lat1)\n",
    "    dLon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "\n",
    "    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    " \n",
    "    return R * c\n",
    "\n",
    "def within(coords):\n",
    "    if coords[0] is None or coords[1] is None:\n",
    "\treturn False\n",
    "    if NAME == \"dmoz\":\n",
    "\treturn float(coords[0]) < 40.796126\n",
    "    # que\n",
    "    elif NAME == \"que\":\n",
    "\tkm = haversine(40.748688, -73.93798, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 3\n",
    "    # sf\n",
    "    elif NAME == \"sfc\":\n",
    "\tkm = haversine(37.779076, -122.397501, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 1.5\n",
    "    # berkeley\n",
    "    elif NAME == \"eby\":\n",
    "\tkm = haversine(37.871454, -122.298115, float(coords[0]), float(coords[1]))\n",
    "\treturn km < 3\n",
    "    # milbrae\n",
    "    #    km = haversine(37.600122, -122.386914, float(coords[0]), float(coords[1]))\n",
    "    # daly city\n",
    "    #    km2 = haversine(37.687915, -122.472452, float(coords[0]), float(coords[1]))\n",
    "    return True\n",
    "\n",
    "coords = craigcr.coords()\n",
    "outside = set()\n",
    "for ci in marked:\n",
    "    \n",
    "    if not within(coords[ci]):\n",
    "\toutside.add(ci)\n",
    "marked = marked.difference(outside)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords = craigcr.coords()\n",
    "sameloc = set()\n",
    "for ci in marked:\n",
    "    if coords[ci][0] is None:\n",
    "\t    continue\n",
    "    if ci in sameloc:\n",
    "\t    continue\n",
    "    # for j,c2 in enumerate(list(marked[type])[i+1:]):\n",
    "    # \tif coords[c2] == coords[c]:\n",
    "    # \t    sameloc.add(i)\n",
    "    # \t    sameloc.add(i+1+j)\n",
    "    for cj,c2 in enumerate(coords):\n",
    "\t    if ci == cj:\n",
    "\t\tcontinue\n",
    "\t    if c2 == coords[ci]:\n",
    "\t\tsameloc.add(ci)\n",
    "\t\tsameloc.add(cj)\n",
    "\t\tbreak\n",
    "# same lat and long is okay ... comment below\n",
    "# but leasebreak piles into same coord\n",
    "# marked[type] = marked[type].difference(sameloc)\n",
    "len(marked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def filter(d, listedby):\n",
    "    ok = bool(listedby) # if sublet (not listedby) then default to suspicious\n",
    "    for w in d:\n",
    "#\tif w in set(['broker', 'fee', 'fully', 'brokerage', 'leasebreak', 'female', 'rentsfnow', 'hivesf', 'heritage', 'realty', 'application', 'yr', 'year', '12', ]):\n",
    "#\t    return False\n",
    "\tif re.compile(\"!!+|leasebreak|dalia|greystar\", re.IGNORECASE).search(w):\n",
    "\t    return False\n",
    "\tif re.compile(\"^(i|me|mine|our|he|she|they|their|we|my|his|her)$\", re.IGNORECASE).search(w):\n",
    "\t    ok = True\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "listedby = craigcr.listedby()\n",
    "filtListedBy = set()\n",
    "for pair in Counter(listedby).iteritems():\n",
    "    if not pair[0]:\n",
    "\tfiltListedBy.add(pair[0])\n",
    "    elif pair[1] == 1:\n",
    "\tif not re.compile(\"no fee|contact|apartments|apts|for all|llc|to view|===+|----+|\\*\\*\\*+|\\.\\.\\.+|xxxx+|best|mark a|marc a|rentals|real|estate\", re.IGNORECASE).search(pair[0]):\n",
    "\t    if len(pair[0].split()) < 5:\n",
    "\t\tfiltListedBy.add(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mark = {}\n",
    "for type in tla:\n",
    "    try:\n",
    "\twith open(pathjoin(odir, 'marker0'), 'r') as f0:\n",
    "\t    mark[type] = max(['0'] + [re.findall(r\"{0}/(\\d+).html\".format(type), line)[-1] for line in f0.readlines() if re.findall(r\"{0}/(\\d+).html\".format(type), line)])\n",
    "    except (IOError, IndexError) as e:\n",
    "\tmark[type] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "links = craigcr.links()\n",
    "listedby = craigcr.listedby()\n",
    "lst = ['%s %s %s' % (' '.join(d[0:50]), links[i], listedby[i] if listedby[i] else \"Actual Person?\") for (i, d) in enumerate(craigcr.docs()) if len(d) > 10  and i in set(marked) and filter(d, listedby[i]) and listedby[i] in filtListedBy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open(pathjoin(odir, 'digest'), 'w+') as f1:\n",
    "    for type in tla:\n",
    "        for listing in lst:\n",
    "\t    try:\n",
    "                id = re.findall(r\"{0}/(?:[^/]+/)*?(\\d+).html\".format(type), listing)[-1]\n",
    "\t\tif id > mark[type]:\n",
    "\t\t    f1.write(listing.encode('ascii', 'ignore') + '\\n\\n')\n",
    "\t    except IndexError as e:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "os.rename(pathjoin(odir, 'marker1'), pathjoin(odir, 'marker0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "dedupe.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
